{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.DEBUG)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install -q ruamel.yaml\n",
    "    !pip install -q tensorboard-plugin-profile\n",
    "    project_path = '/content/drive/MyDrive/Colab Projects/quantumflow'\n",
    "except ImportError:\n",
    "    project_path = os.path.expanduser('~/quantumflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(project_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "\n",
    "import quantumflow\n",
    "\n",
    "experiment = 'resnets'\n",
    "run_name = 'derivative'\n",
    "\n",
    "base_dir = os.path.join(project_path, \"experiments\", experiment)\n",
    "params = quantumflow.utils.load_yaml(os.path.join(base_dir, 'hyperparams.yaml'))[run_name]\n",
    "run_dir = os.path.join(base_dir, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=\"$base_dir\" --load_fast=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = quantumflow.instantiate(params['dataset_train'], run_dir=run_dir)\n",
    "dataset_train.build()\n",
    "\n",
    "dataset_validate = quantumflow.instantiate(params['dataset_validate'], run_dir=run_dir)\n",
    "dataset_validate.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.profiler.experimental.server.start(6009)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(params['seed'])\n",
    "\n",
    "model = quantumflow.instantiate(params['model'], run_dir=run_dir, dataset=dataset_train)\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "for var in model.variables:\n",
    "    print(f\"{np.prod(var.shape):>4d}\", var.name, var.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantumflow.utils import anim_plot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tree\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features = tree.map_structure(lambda feature: feature[10:11], dataset_validate.features)\n",
    "sample_targets = tree.map_structure(lambda target: target[10:11], dataset_validate.targets)\n",
    "sample_targets_pred = model(sample_features)\n",
    "sample = tree.map_structure(lambda target, target_pred: (target[0], target_pred.numpy()[0]), sample_targets, sample_targets_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_path, (target, target_pred) in tree.flatten_with_path_up_to(sample_targets, sample):\n",
    "    target_name = '/'.join(target_path)\n",
    "    \n",
    "    if np.squeeze(target).shape == dataset_validate.x.shape:\n",
    "\n",
    "        plt.figure(figsize=(20, 3))\n",
    "        plt.plot(dataset_validate.x, target, 'k:')\n",
    "        plt.plot(dataset_validate.x, np.squeeze(target_pred))\n",
    "        plt.title(target_name)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{target_name}: {target_pred} ({target})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model = model if not hasattr(model, 'base_model') else model.base_model\n",
    "visualize_params = params['model'] if not hasattr(model, 'base_model') else params['model']['base_model']\n",
    "\n",
    "import quantumflow.xdiff\n",
    "\n",
    "latents = visualize_model.layers[0](sample_features)\n",
    "x, x_inputs, inputs = visualize_model.layers[1](latents['density'])\n",
    "x = visualize_model.layers[2](x)\n",
    "\n",
    "self = visualize_model.layers[3]\n",
    "\n",
    "x_token = self.x_token # (d_model)\n",
    "for shape in tf.unstack(tf.shape(x))[:-2]:\n",
    "    x_token = tf.repeat(tf.expand_dims(x_token, axis=-3), shape, axis=-3) # (..., latent_size, d_model)\n",
    "x_token = tf.repeat(x_token, tf.shape(x)[-2], axis=0)\n",
    "\n",
    "xdiff = quantumflow.xdiff.get_xdiff(x, x, visualize_params['scale'], visualize_params['K'])\n",
    "xdiff_cross = quantumflow.xdiff.get_xdiff(x, x_inputs, visualize_params['scale'], visualize_params['K'])\n",
    "\n",
    "inputs = tf.concat([\n",
    "    inputs, \n",
    "    quantumflow.xdiff.positional_encoding(inputs, visualize_params['K_input'])\n",
    "], axis=-1) # (..., x1_size, x2_size, x_features)\n",
    "    \n",
    "latents = x_token #self.x_token_layer(xdiff, xdiff_cross)\n",
    "\n",
    "layers = []\n",
    "for r in range(visualize_params['num_repeats']):\n",
    "    for i in range(visualize_params['num_layers']):\n",
    "        layers.append(self.enc_layers[r][i])\n",
    "    layers.append(self.cross_enc_layers[r])\n",
    "\n",
    "for i in range(visualize_params['num_layers']):\n",
    "    layers.append(self.enc_layers[visualize_params['num_repeats']][i])\n",
    "\n",
    "layers.append(self.layernorm)\n",
    "\n",
    "for layer in self.pre_final_layers:\n",
    "    layers.append(layer)\n",
    "\n",
    "layers.append(self.final_layer)\n",
    "\n",
    "    \n",
    "for i in range(latents.shape[2]):\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.plot(latents[0, :, i, :])\n",
    "    plt.title(f\"Latents {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}\")\n",
    "    plt.show()\n",
    "                \n",
    "for self in layers:\n",
    "    print(self.name)\n",
    "    if 'encoder' in self.name:\n",
    "        if 'cross' in self.name:\n",
    "            \n",
    "            inp = inputs\n",
    "            lat = self.layernorm1(latents)\n",
    "                \n",
    "            for i in range(lat.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(lat[0, :, i, :])\n",
    "                plt.title(f\"Normalized Latents\")\n",
    "                plt.show()\n",
    "                \n",
    "            plt.figure(figsize=(20, 3))\n",
    "            plt.plot(inp[0, 0, :, :])\n",
    "            plt.title('Normalized Inputs')\n",
    "            plt.show()\n",
    "                \n",
    "            attn_output, attention = self.mha(lat, inp, inp, xdiff_cross, mask=None)  # (..., latent_size, d_model)\n",
    "\n",
    "            for i in range(attention.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.imshow(attention[0, :, i, 0, :], norm=matplotlib.colors.Normalize(vmin=0, vmax=0.01, clip=False), aspect=1.0)\n",
    "                plt.show()\n",
    "                print(f'Attention Map {np.std(attention[0, :, i, 0, :]):.3f}')\n",
    "                \n",
    "            for i in range(attn_output.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(attn_output[0, :, i, :])\n",
    "                plt.title(f'Attention Output {np.mean(attn_output[0, :, i, :]):.3f} {np.std(attn_output[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            attn_output = self.dropout1(attn_output, training=True)\n",
    "            \n",
    "            latents = latents + attn_output\n",
    "            \n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip Attn Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            lat = self.layernorm2(latents)  # (..., latent_size, d_model)\n",
    "            ffn_output = self.ffn[1](self.ffn[0](lat))  # (..., input_size, d_model)\n",
    "            ffn_output = self.dropout2(ffn_output, training=True)\n",
    "\n",
    "            latents = latents + ffn_output\n",
    "            \n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip FFN Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "        else:\n",
    "            lat = self.layernorm1(latents)  # (..., input_size, d_model)\n",
    "                \n",
    "            for i in range(lat.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(lat[0, :, i, :])\n",
    "                plt.title('Normalized Latents')\n",
    "                plt.show()\n",
    "\n",
    "            attn_output, attention = self.mha(lat, lat, lat, xdiff, mask=None)  # (..., input_size, d_model)\n",
    "\n",
    "            if attention.shape[-1] > 1:\n",
    "                for i in range(attention.shape[2]):\n",
    "\n",
    "                    plt.figure(figsize=(20, 3))\n",
    "                    plt.imshow(attention[0, :, i, 0, :], norm=matplotlib.colors.Normalize(vmin=0, vmax=0.01, clip=False), aspect=1.0)\n",
    "                    plt.show()\n",
    "                    print(f'Attention Map {np.std(attention[0, :, i, 0, :]):.3f}')\n",
    "            else:\n",
    "                print(attention[0, :, :, 0, 0].numpy())\n",
    "            \n",
    "            for i in range(attn_output.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(attn_output[0, :, i, :])\n",
    "                plt.title(f'Attention Output {np.mean(attn_output[0, :, i, :]):.3f} {np.std(attn_output[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            attn_output = self.dropout1(attn_output, training=True)\n",
    "            \n",
    "            latents = latents + attn_output\n",
    "\n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip Attn Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "                \n",
    "            lat = self.layernorm2(latents)  # (..., input_size, d_model)\n",
    "            ffn_output = self.ffn[1](self.ffn[0](lat))  # (..., input_size, d_model)\n",
    "            ffn_output = self.dropout2(ffn_output, training=True)\n",
    "            \n",
    "            latents = latents + ffn_output\n",
    "            \n",
    "            for i in range(latents.shape[2]):\n",
    "                plt.figure(figsize=(20, 3))\n",
    "                plt.plot(latents[0, :, i, :])\n",
    "                plt.title(f'Skip FFN Output {np.mean(latents[0, :, i, :]):.3f} {np.std(latents[0, :, i, :]):.3f}')\n",
    "                plt.show()\n",
    "    else:\n",
    "        latents = self(latents)\n",
    "\n",
    "kinetic_energy_density = tf.reduce_sum(latents[..., 0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kinetic_energy_density[0] - sample_targets_pred['kinetic_energy_density'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise YouShallNotPass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = quantumflow.instantiate(params['optimizer'])\n",
    "\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    loss=params['loss'], \n",
    "    loss_weights=params.get('loss_weights', None), \n",
    "    metrics=params.get('metrics', None)\n",
    ")\n",
    "\n",
    "\n",
    "if params.get('load_checkpoint', None) is not None:\n",
    "    model.load_weights(os.path.join(data_dir, params['load_checkpoint']))\n",
    "    if params['fit'].get('verbose', 0) > 0:\n",
    "        print(\"loading weights from \", os.path.join(data_dir, params['load_checkpoint']))\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "\n",
    "if run_dir is not None and params.get('checkpoint', False):\n",
    "    checkpoint_params = params['checkpoint'].copy()\n",
    "    checkpoint_params['filepath'] = os.path.join(run_dir, checkpoint_params.pop('filename', 'weights.{epoch:05d}.hdf5'))\n",
    "    checkpoint_params['verbose'] = checkpoint_params.get('verbose', min(1, params['fit'].get('verbose', 1)))\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(**checkpoint_params))\n",
    "\n",
    "\n",
    "if 'tensorboard' in params:\n",
    "    callbacks.append(\n",
    "        quantumflow.instantiate(params['tensorboard'], log_dir=run_dir, learning_rate=optimizer.learning_rate))\n",
    "\n",
    "\n",
    "model.fit(x=dataset_train.features, \n",
    "          y=dataset_train.targets, \n",
    "          callbacks=callbacks,\n",
    "          validation_data=(dataset_validate.features, dataset_validate.targets) if dataset_validate is not None else None,\n",
    "          **params['fit'])\n",
    "\n",
    "if params['save'] is True:\n",
    "    save_model = getattr(model, params['save_model']) if not params.get('save_model', 'self') == 'self' else model\n",
    "    save_model.save(os.path.join(run_dir, 'saved_model'), include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnjoWzd/gc4HbCp8FvSubN",
   "collapsed_sections": [],
   "name": "train_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
